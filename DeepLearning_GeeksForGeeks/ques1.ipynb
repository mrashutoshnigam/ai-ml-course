{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d7cd21",
   "metadata": {},
   "source": [
    "Implement the Sparse auto-encoder and the Contractive Auto-encoder. Use the MNIST digit dataset for\n",
    "training your network. Use the U-Net auto-encoder architecture for encoder and decoder without skip\n",
    "connections. Let E be the trained encoder and D be the trained decoder, h = E(I) be the embedding of\n",
    "an image I and let ˆI = D(h) be the output of the decoder.\n",
    "(a) (20 points) Plot the t-sne (use inbuilt function) on the embeddings obtained using the respective\n",
    "auto-encoders. Color the clusters using the respective ground-truth class labels.\n",
    "(b) (20 points) Randomly take two images I1 and I2 from two different digit classes. Let h1 = E(I1)\n",
    "and h2 = E(I2) be the embeddings for these images, respectively. Construct another image Iα =\n",
    "αI1+(1−α)I2 for α = 0, 0.2, 0.4, 0.6, 0.8, 1. Find the embedding hα of this image Iα for all values of α\n",
    "by passing it through the encoder. Also, consider the approximate embedding h\n",
    "′\n",
    "α = αh1 + (1−α)h2\n",
    "by using directly the embeddings of the images I1 and I2. Also, find ˆIα = D(hα) and ˆI\n",
    "′\n",
    "α = D(h\n",
    "′\n",
    "α).\n",
    "Plot the images ˆIα and ˆI\n",
    "′\n",
    "α side by side for different values of α. Do this for 20 pairs (I1, I2). Report\n",
    "PSNR between ˆIα and ˆI\n",
    "′\n",
    "α and find ∥hˆ\n",
    "α − hˆ′\n",
    "α∥2 for all values of alpha.\n",
    "(c) (20 points) After training the autoencoders, you want to check if the embeddings of different digits\n",
    "are different and embeddings within a class are similar. For this purpose, you propose to perform\n",
    "the classification of the digits based on the embeddings obtained by the encoders and check the\n",
    "accuracy of classifications for each of the Auto-encoder. Report the classification accuracy for each\n",
    "of the AE and report which one is better. Use any inbuilt classifier to solve the classification\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418cfc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: seaborn in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow numpy pandas matplotlib scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd618d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db679a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "sparse_lambda = 1e-3  # Sparsity penalty\n",
    "contractive_lambda = 1e-4  # Contractive penalty\n",
    "rho = 0.05  # Target sparsity\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)  # Shape: (60000, 28, 28, 1)\n",
    "x_train = (x_train - 0.1307) / 0.3081  # Normalize as per PyTorch example\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9dfbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    x = layers.Conv2D(num_filters, 3, padding='valid')(inputs)\n",
    "    x = layers.Activation('relu')(x)    \n",
    "    x = layers.Conv2D(num_filters, 3, padding='valid')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=2)(x)\n",
    "    return x\n",
    "    \n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding='valid')(inputs)\n",
    "    x = layers.Conv2D(num_filters, 3, padding='valid')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(num_filters, 3, padding='valid')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "class UNetAutoEncoderArchitecture():\n",
    "    def build_encoder(self, input_shape=(28, 28, 1)):\n",
    "        inputs= layers.Input(shape=input_shape)\n",
    "        x = encoder_block(inputs, 64)\n",
    "        x = encoder_block(x, 128)\n",
    "        x = encoder_block(x, 256)        \n",
    "        return models.Model(inputs=inputs, outputs=x, name='encoder')\n",
    "    \n",
    "    def build_decoder(self, input_shape=(128,)):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = decoder_block(inputs, None, 256)\n",
    "        x = decoder_block(x, None, 128)\n",
    "        x = decoder_block(x, None, 64)\n",
    "        return models.Model(inputs=inputs, outputs=x, name='decoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5b2d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(models.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = UNetAutoEncoderArchitecture().build_encoder(input_shape)\n",
    "        self.decoder = UNetAutoEncoderArchitecture().build_decoder()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(128, activation='relu')\n",
    "        self.output_layer = layers.Dense(np.prod(input_shape), activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContractiveAutoencoder(models.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(ContractiveAutoencoder, self).__init__()\n",
    "        self.encoder = UNetAutoEncoderArchitecture().build_encoder()\n",
    "        self.decoder = UNetAutoEncoderArchitecture().build_decoder()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(128, activation='relu')\n",
    "        self.output_layer = layers.Dense(np.prod(input_shape), activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b83fd024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_loss(y_true, y_pred):\n",
    "    mse_loss = tf.reduce_mean(keras.losses.MeanSquaredError()(y_true, y_pred))\n",
    "    hidden_layer_output = encoder(y_true)\n",
    "    mean_activation = tf.reduce_mean(hidden_layer_output, axis=0)\n",
    "\n",
    "    kl_divergence = tf.reduce_sum(sparsity_level * tf.math.log(sparsity_level / (mean_activation + 1e-10)) +\n",
    "                                  (1 - sparsity_level) * tf.math.log((1 - sparsity_level) / (1 - mean_activation + 1e-10)))\n",
    "\n",
    "    return mse_loss + lambda_sparse * kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d893f2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv2d_transpose_1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sparse_ae \u001b[38;5;241m=\u001b[39m \u001b[43mSparseAutoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m sparse_ae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate), loss\u001b[38;5;241m=\u001b[39msparse_loss)\n\u001b[0;32m      3\u001b[0m sparse_ae\u001b[38;5;241m.\u001b[39mfit(train_dataset, epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mSparseAutoencoder.__init__\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(SparseAutoencoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m UNetAutoEncoderArchitecture()\u001b[38;5;241m.\u001b[39mbuild_encoder(input_shape)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mUNetAutoEncoderArchitecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m, in \u001b[0;36mUNetAutoEncoderArchitecture.build_decoder\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_decoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m,)):\n\u001b[0;32m     28\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m decoder_block(x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m decoder_block(x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m64\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mdecoder_block\u001b[1;34m(inputs, skip_features, num_filters)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecoder_block\u001b[39m(inputs, skip_features, num_filters):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2DTranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(num_filters, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mActivation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "File \u001b[1;32me:\\AI\\ai-ml-course\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\AI\\ai-ml-course\\venv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:202\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmin_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv2d_transpose_1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "sparse_ae = SparseAutoencoder(input_shape=(28, 28, 1))\n",
    "sparse_ae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=sparse_loss)\n",
    "sparse_ae.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26466c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original images\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Reconstructed images\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
