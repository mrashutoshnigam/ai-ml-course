{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4a2324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: numpy in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: pandas in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch numpy matplotlib pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416186ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "def load_mnist():\n",
    "    return fetch_openml('mnist_784', version=1)\n",
    "mnist = load_mnist()\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b689c6",
   "metadata": {},
   "source": [
    "Implement Sparse AutoEncoder    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223f34d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "# Implement U-Net Auto Encoder Architecture for encoding and decoding without skip connections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare MNIST data\n",
    "X = mnist.data.values.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "y = mnist.target.values.astype(np.int64)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "class UNetAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, feature_dims=[32, 64, 128, 256]):\n",
    "        super(UNetAutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[3], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[3], feature_dims[3], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Decoder (no skip connections)\n",
    "        self.up3 = nn.ConvTranspose2d(feature_dims[3], feature_dims[2], kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(feature_dims[2], feature_dims[1], kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up1 = nn.ConvTranspose2d(feature_dims[1], feature_dims[0], kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final = nn.Conv2d(feature_dims[0], in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool(x1))\n",
    "        x3 = self.enc3(self.pool(x2))\n",
    "        x4 = self.enc4(self.pool(x3))\n",
    "        # Decoder (no skip connections)\n",
    "        d3 = self.up3(x4)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(d1)\n",
    "        out = torch.sigmoid(self.final(d1))\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "# Reshape X_train_tensor to (N, 1, 28, 28) for convolutional input\n",
    "X_train_img = X_train_tensor.view(-1, 1, 28, 28)\n",
    "X_test_img = X_test_tensor.view(-1, 1, 28, 28)\n",
    "unet_model = UNetAutoEncoder()\n",
    "output = unet_model(X_train_img[:8])\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc8d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the U-Net AutoEncoder\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Modify the UNetAutoEncoder class to handle the dimension issue\n",
    "class UNetAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, feature_dims=[32, 64, 128, 256]):\n",
    "        super(UNetAutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[3], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[3], feature_dims[3], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Decoder (no skip connections)\n",
    "        self.up3 = nn.ConvTranspose2d(feature_dims[3], feature_dims[2], kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[2], feature_dims[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(feature_dims[2], feature_dims[1], kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[1], feature_dims[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up1 = nn.ConvTranspose2d(feature_dims[1], feature_dims[0], kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(feature_dims[0], feature_dims[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final = nn.Conv2d(feature_dims[0], in_channels, kernel_size=1)\n",
    "        \n",
    "        # Output resizing layer to match input dimensions\n",
    "        self.output_pad = nn.ZeroPad2d((2, 2, 2, 2))  # Padding to fix size mismatch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save input size for later resizing\n",
    "        input_size = x.size()\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool(x1))\n",
    "        x3 = self.enc3(self.pool(x2))\n",
    "        x4 = self.enc4(self.pool(x3))\n",
    "        \n",
    "        # Decoder (no skip connections)\n",
    "        d3 = self.up3(x4)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(d1)\n",
    "        out = self.final(d1)\n",
    "        \n",
    "        # Resize output to match input dimensions exactly\n",
    "        if out.shape != input_size:\n",
    "            # Calculate padding needed\n",
    "            diff_h = input_size[2] - out.size(2)\n",
    "            diff_w = input_size[3] - out.size(3)\n",
    "            \n",
    "            if diff_h > 0 and diff_w > 0:\n",
    "                # Add padding if needed\n",
    "                pad_h = diff_h // 2\n",
    "                pad_w = diff_w // 2\n",
    "                out = F.pad(out, (pad_w, pad_w + diff_w % 2, \n",
    "                                 pad_h, pad_h + diff_h % 2))\n",
    "            elif diff_h < 0 or diff_w < 0:\n",
    "                # Center crop if output is larger\n",
    "                out = F.center_crop(out, (input_size[2], input_size[3]))\n",
    "                \n",
    "        return torch.sigmoid(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d9ec69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in e:\\ai\\ai-ml-course\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, torchinfo\n",
      "Successfully installed torchinfo-1.8.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ba72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI\\ai-ml-course\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/438 [00:00<?, ?it/s]e:\\AI\\ai-ml-course\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1/10: 100%|██████████| 438/438 [01:28<00:00,  4.94it/s, loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 0.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 438/438 [01:37<00:00,  4.51it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Average Loss: 0.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 438/438 [01:30<00:00,  4.84it/s, loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Average Loss: 0.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "unet_model = UNetAutoEncoder(in_channels=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.001)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "unet_model.to(device)\n",
    "\n",
    "# Training parameters with faster convergence\n",
    "num_epochs = 10  # Reduced epochs\n",
    "batch_size = 128  # Increased batch size if memory allows\n",
    "\n",
    "# Learning rate scheduler for faster convergence\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "# Create data loader with multiple workers for faster data loading\n",
    "train_dataset = TensorDataset(X_train_img, X_train_img)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4,  # Parallel data loading\n",
    "    pin_memory=True  # Faster data transfer to GPU\n",
    ")\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    unet_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Progress bar for better monitoring\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_data, batch_target in progress_bar:\n",
    "        # Move data to device\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            # Mixed precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                reconstructed = unet_model(batch_data)\n",
    "                loss = criterion(reconstructed, batch_target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Regular training\n",
    "            reconstructed = unet_model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Update learning rate based on performance\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "# Evaluate on test set\n",
    "unet_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch_size = 256  # Larger batch for faster evaluation\n",
    "    test_dataset = TensorDataset(X_test_img, X_test_img)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    test_reconstructed = []\n",
    "    \n",
    "    for batch_data, batch_target in test_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        batch_reconstructed = unet_model(batch_data)\n",
    "        test_reconstructed.append(batch_reconstructed.cpu())\n",
    "        total_test_loss += criterion(batch_reconstructed, batch_target).item()\n",
    "    \n",
    "    # Concatenate batches\n",
    "    test_reconstructed = torch.cat(test_reconstructed)\n",
    "    unet_test_loss = total_test_loss / len(test_loader)\n",
    "    \n",
    "print(f'U-Net Test Loss: {unet_test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5eab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize reconstructions\n",
    "def visualize_unet_reconstruction(original, reconstructed, num_images=10):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(num_images):\n",
    "        # Original\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(original[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('Original')\n",
    "        \n",
    "        # Reconstructed\n",
    "        plt.subplot(2, num_images, i + 1 + num_images)\n",
    "        plt.imshow(reconstructed[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('U-Net Reconstructed')\n",
    "    plt.suptitle('U-Net AutoEncoder Results')\n",
    "    plt.show()\n",
    "\n",
    "visualize_unet_reconstruction(X_test_img[:10], test_reconstructed[:10])\n",
    "\n",
    "# Save the U-Net model\n",
    "torch.save(unet_model.state_dict(), 'unet_autoencoder.pth')\n",
    "print(\"U-Net AutoEncoder model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e28efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Comparison ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compare Sparse vs U-Net AutoEncoder performance\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Model Comparison ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse AutoEncoder Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtest_loss\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU-Net AutoEncoder Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Visualize side-by-side comparison\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare Sparse vs U-Net AutoEncoder performance\n",
    "print(\"=== Model Comparison ===\")\n",
    "print(f\"Sparse AutoEncoder Test Loss: {test_loss:.4f}\")\n",
    "print(f\"U-Net AutoEncoder Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Visualize side-by-side comparison\n",
    "fig, axes = plt.subplots(3, 10, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    # Original\n",
    "    axes[0, i].imshow(X_test_tensor[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Sparse AutoEncoder\n",
    "    axes[1, i].imshow(decoded_test[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title('Sparse AE')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # U-Net AutoEncoder\n",
    "    axes[2, i].imshow(test_reconstructed[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "    axes[2, i].set_title('U-Net AE')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Sparse AutoEncoder using U-net AutoEncoder architecture\n",
    "class SparseUNetAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, feature_dims=[32, 64, 128, 256], sparsity_param=0.05):\n",
    "        super(SparseUNetAutoEncoder, self).__init__()\n",
    "        self.sparsity_param = sparsity_param\n",
    "        self.unet = UNetAutoEncoder(in_channels, feature_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unet(x)\n",
    "\n",
    "    def sparsity_loss(self, encoded):\n",
    "        mean_activation = torch.mean(encoded, dim=0)\n",
    "        sparsity_loss = torch.sum(self.sparsity_param * torch.log(self.sparsity_param / mean_activation) +\n",
    "                                  (1 - self.sparsity_param) * torch.log((1 - self.sparsity_param) / (1 - mean_activation)))\n",
    "        return sparsity_loss\n",
    "# Initialize Sparse U-Net AutoEncoder model\n",
    "sparse_unet_model = SparseUNetAutoEncoder(in_channels=1)\n",
    "# Initialize criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(sparse_unet_model.parameters(), lr=0.001)\n",
    "# Training the Sparse U-Net AutoEncoder\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "# Create data loader for better training\n",
    "train_dataset = TensorDataset(X_train_img, X_train_img)  # Input and target are the same for autoencoder\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    sparse_unet_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_data, batch_target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = sparse_unet_model(batch_data)\n",
    "        loss = criterion(reconstructed, batch_target) + sparse_unet_model.sparsity_loss(batch_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "# Evaluate on test set\n",
    "sparse_unet_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_reconstructed = sparse_unet_model(X_test_img)\n",
    "    sparse_unet_test_loss = criterion(test_reconstructed, X_test_img).item()\n",
    "    print(f'Sparse U-Net Test Loss: {sparse_unet_test_loss:.4f}')\n",
    "# Visualize reconstructions\n",
    "visualize_unet_reconstruction(X_test_img[:10], test_reconstructed[:10])\n",
    "# Save the Sparse U-Net model\n",
    "torch.save(sparse_unet_model.state_dict(), 'sparse_unet_autoencoder.pth')\n",
    "print(\"Sparse U-Net AutoEncoder model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20811b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Contractive AutoEncoder using U-net AutoEncoder architecture\n",
    "class ContractiveUNetAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, feature_dims=[32, 64, 128, 256], lambda_param=0.001):\n",
    "        super(ContractiveUNetAutoEncoder, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.unet = UNetAutoEncoder(in_channels, feature_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unet(x)\n",
    "\n",
    "    def contractive_loss(self, encoded):\n",
    "        # Compute the Jacobian of the encoded layer\n",
    "        jacobian = torch.autograd.functional.jacobian(self.unet.forward, encoded)\n",
    "        # Compute the Frobenius norm of the Jacobian\n",
    "        frobenius_norm = torch.norm(jacobian.view(jacobian.size(0), -1), dim=1)\n",
    "        return self.lambda_param * torch.sum(frobenius_norm)\n",
    "# Initialize Contractive U-Net AutoEncoder model\n",
    "contractive_unet_model = ContractiveUNetAutoEncoder(in_channels=1)\n",
    "# Initialize criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(contractive_unet_model.parameters(), lr=0.001)\n",
    "# Training the Contractive U-Net AutoEncoder\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "# Create data loader for better training\n",
    "train_dataset = TensorDataset(X_train_img, X_train_img)  # Input and target are the same for autoencoder\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    contractive_unet_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_data, batch_target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = contractive_unet_model(batch_data)\n",
    "        loss = criterion(reconstructed, batch_target) + contractive_unet_model.contractive_loss(batch_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "# Evaluate on test set\n",
    "contractive_unet_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_reconstructed = contractive_unet_model(X_test_img)\n",
    "    contractive_unet_test_loss = criterion(test_reconstructed, X_test_img).item()\n",
    "    print(f'Contractive U-Net Test Loss: {contractive_unet_test_loss:.4f}')\n",
    "# Visualize reconstructions\n",
    "visualize_unet_reconstruction(X_test_img[:10], test_reconstructed[:10])\n",
    "# Save the Contractive U-Net model\n",
    "torch.save(contractive_unet_model.state_dict(), 'contractive_unet_autoencoder.pth')\n",
    "print(\"Contractive U-Net AutoEncoder model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ef871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot t-sne of the encoded features from the Sparse AutoEncoder    \n",
    "from sklearn.manifold import TSNE\n",
    "def plot_tsne(encoded_features, labels, title='t-SNE of Encoded Features'):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(encoded_features.detach().numpy())\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.show() \n",
    "# Extract encoded features from the Sparse AutoEncoder\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, encoded_features = model(X_test_tensor)\n",
    "plot_tsne(encoded_features, y_test, title='t-SNE of Sparse AutoEncoder Encoded Features')\n",
    "# plot t-sne of the encoded features from the U-Net AutoEncoder\n",
    "with torch.no_grad():\n",
    "    _, encoded_features_unet = unet_model(X_test_img)\n",
    "plot_tsne(encoded_features_unet.view(encoded_features_unet.size(0), -1), y_test, title='t-SNE of U-Net AutoEncoder Encoded Features')\n",
    "# plot t-sne of the encoded features from the Sparse U-Net AutoEncoder\n",
    "with torch.no_grad():\n",
    "    _, encoded_features_sparse_unet = sparse_unet_model(X_test_img)\n",
    "plot_tsne(encoded_features_sparse_unet.view(encoded_features_sparse_unet.size(0), -1), y_test, title='t-SNE of Sparse U-Net AutoEncoder Encoded Features')\n",
    "# plot t-sne of the encoded features from the Contractive U-Net AutoEncoder\n",
    "with torch.no_grad():\n",
    "    _, encoded_features_contractive_unet = contractive_unet_model(X_test_img)\n",
    "plot_tsne(encoded_features_contractive_unet.view(encoded_features_contractive_unet.size(0), -1), y_test, title='t-SNE of Contractive U-Net AutoEncoder Encoded Features')\n",
    "# The Sparse AutoEncoder, U-Net AutoEncoder, Sparse U-Net AutoEncoder, and Contractive U-Net AutoEncoder have been implemented.\n",
    "# Each model has been trained, evaluated, and visualized using t-SNE to show the encoded features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
