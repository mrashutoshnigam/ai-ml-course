{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ed0aa1",
   "metadata": {},
   "source": [
    " 4. Build a spam classifier (a more challenging exercise):\n",
    " • Download examples of spam and ham from Apache SpamAssassin’s public\n",
    " datasets.\n",
    " • Unzip the datasets and familiarize yourself with the data format.\n",
    " • Split the datasets into a training set and a test set.\n",
    " • Write a data preparation pipeline to convert each email into a feature vector.\n",
    " Your preparation pipeline should transform an email into a (sparse) vector that\n",
    " indicates the presence or absence of each possible word. For example, if all\n",
    " emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email\n",
    " “Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1]\n",
    " (meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is\n",
    " present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\n",
    " each word.\n",
    " You may want to add hyperparameters to your preparation pipeline to control\n",
    " whether or not to strip off email headers, convert each email to lowercase,\n",
    " remove punctuation, replace all URLs with “URL,” replace all numbers with\n",
    " “NUMBER,” or even perform stemming (i.e., trim off word endings; there are\n",
    " Python libraries available to do this).\n",
    " Finally, try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92775dee",
   "metadata": {},
   "source": [
    "https://grok.com/share/c2hhcmQtMw%3D%3D_3490c19f-cb97-4031-8832-55d63b95a7b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "55bf6f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn-intelex in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (2025.7.0)\n",
      "Requirement already satisfied: bz2file in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (0.98)\n",
      "Requirement already satisfied: nltk in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: daal==2025.7.0 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from scikit-learn-intelex) (2025.7.0)\n",
      "Requirement already satisfied: tbb==2022.* in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from daal==2025.7.0->scikit-learn-intelex) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from tbb==2022.*->daal==2025.7.0->scikit-learn-intelex) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: click in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in e:\\ai\\ai-ml-course\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn numpy pandas scikit-learn-intelex bz2file nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cae3f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ed7d327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3e31072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'data\\\\raw', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "12329fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def download_file(url):\n",
    "    a = urlparse(url)\n",
    "    path = os.path.basename(a.path)\n",
    "    print(path)\n",
    "    file_path = os.path.join('data\\\\raw', path)\n",
    "    if os.path.isfile(file_path):\n",
    "        return\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "debef10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(url):\n",
    "    a = urlparse(url)\n",
    "    path = os.path.basename(a.path)\n",
    "    file_path = os.path.join('data\\\\raw', path)\n",
    "    extract_folder = os.path.join('data/ham', path.replace('.tar.bz2', ''))\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "    # Only extract if the folder is empty\n",
    "    if not os.listdir(extract_folder):\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            tar.extractall(extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2a9ad0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url, list_ham, list_spam):\n",
    "    a = urlparse(url)\n",
    "    path = os.path.basename(a.path)\n",
    "    folder_name = path.replace('.tar.bz2', '')\n",
    "    extract_folder = os.path.join('data\\\\ham', folder_name)\n",
    "    # List all files in the extracted folder\n",
    "    for root, dirs, files in os.walk(extract_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if 'spam' in folder_name:\n",
    "                list_spam.append(file_path)\n",
    "            else:\n",
    "                list_ham.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8279a018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20021010_easy_ham.tar.bz2\n",
      "20030228_easy_ham.tar.bz2\n",
      "20030228_easy_ham_2.tar.bz2\n",
      "20021010_spam.tar.bz2\n",
      "20030228_spam.tar.bz2\n",
      "20030228_spam_2.tar.bz2\n"
     ]
    }
   ],
   "source": [
    "ham_url_ham_easy=['https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2','https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2','https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2']\n",
    "ham_url_spam_easy=['https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2','https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2','https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2']\n",
    "\n",
    "ham_files=[]\n",
    "spam_files=[]\n",
    "\n",
    "for ham in ham_url_ham_easy + ham_url_spam_easy:\n",
    "    download_file(ham)\n",
    "    extract_file(ham)\n",
    "    load_data(ham,ham_files,spam_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8925915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Ham files :6453, No of Spam Files: 2400\n"
     ]
    }
   ],
   "source": [
    "print(f'No of Ham files :{len(ham_files)}, No of Spam Files: {len(spam_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "54b25679",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = ham_files + spam_files\n",
    "all_labels = [0] * len(ham_files) + [1] * len(spam_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0d646f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "79e04876",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_files, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f546dc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 7082 samples\n",
      "Test set size: 1771 samples\n",
      "Number of ham in training set: 5162\n",
      "Number of spam in training set: 1920\n",
      "Number of ham in test set: 1291\n",
      "Number of spam in test set: 480\n"
     ]
    }
   ],
   "source": [
    "# Output split sizes for verification\n",
    "print(f'\\nTraining set size: {len(X_train)} samples')\n",
    "print(f'Test set size: {len(X_test)} samples')\n",
    "print(f'Number of ham in training set: {y_train.count(0)}')\n",
    "print(f'Number of spam in training set: {y_train.count(1)}')\n",
    "print(f'Number of ham in test set: {y_test.count(0)}')\n",
    "print(f'Number of spam in test set: {y_test.count(1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c9494",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221ac99",
   "metadata": {},
   "source": [
    "Read Email and Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5d7388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from email.parser import Parser\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0e2c3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_content(email_path, strip_headers=False):\n",
    "    with open(email_path, 'r', encoding='latin-1') as f:\n",
    "        text = f.read()\n",
    "    parser = Parser()\n",
    "    msg = parser.parsestr(text)\n",
    "    if strip_headers:\n",
    "        body = ''\n",
    "        if msg.is_multipart():\n",
    "            for part in msg.walk():\n",
    "                ctype = part.get_content_type()\n",
    "                cdisp = str(part.get('Content-Disposition'))\n",
    "                if ctype == 'text/plain' and 'attachment' not in cdisp:\n",
    "                    body += part.get_payload(decode=True).decode('latin-1', errors='ignore')\n",
    "        else:\n",
    "            body = msg.get_payload(decode=True).decode('latin-1', errors='ignore')\n",
    "        return body\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c952985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts=[],[]\n",
    "for path in X_train:\n",
    "    train_texts.append(get_email_content(path))\n",
    "\n",
    "for path in X_test:\n",
    "    test_texts.append(get_email_content(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "070d87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analyzer(lowercase=True, remove_punct=True, replace_url=True, replace_num=True, stemming=False):\n",
    "    def analyzer_func(text):\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if replace_url:\n",
    "            text = re.sub(r'(http|https|www)\\S+', 'URL', text)\n",
    "        if replace_num:\n",
    "            text = re.sub(r'\\d+', 'NUMBER', text)\n",
    "        if remove_punct:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = text.split()\n",
    "        if stemming:\n",
    "            words = [stemmer.stem(word) for word in words if word]\n",
    "        return words\n",
    "    return analyzer_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21a7c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_headers = False  # Keep headers for better spam indicators\n",
    "lowercase = True\n",
    "remove_punct = True\n",
    "replace_url = True\n",
    "replace_num = True\n",
    "stemming = True\n",
    "binary = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb636f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = get_analyzer(lowercase=lowercase, remove_punct=remove_punct, replace_url=replace_url, \n",
    "                        replace_num=replace_num, stemming=stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6a06d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ddb3ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=analyzer, binary=binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f61ece97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "X_train_vec = vectorizer.fit_transform(train_texts)\n",
    "X_test_vec = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7dedf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training and evaluating classifiers\n",
    "classifiers = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'SVC': SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "87d7346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB - Accuracy: 0.9701, Precision: 0.9954, Recall: 0.8938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI\\ai-ml-course\\.venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:29: UserWarning: `sklearn.utils.parallel.Parallel` needs to be used in conjunction with `sklearn.utils.parallel.delayed` instead of `joblib.delayed` to correctly propagate the scikit-learn configuration to the joblib workers.\n",
      "  warnings.warn(\n",
      "e:\\AI\\ai-ml-course\\.venv\\lib\\site-packages\\sklearnex\\utils\\parallel.py:37: UserWarning: `sklearn.utils.parallel.delayed` should be used with `sklearn.utils.parallel.Parallel` to make it possible to propagate the scikit-learn configuration of the current thread to the joblib workers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression - Accuracy: 0.9944, Precision: 0.9958, Recall: 0.9833\n",
      "SVC - Accuracy: 0.9870, Precision: 0.9893, Recall: 0.9625\n"
     ]
    }
   ],
   "source": [
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f1f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
